___________________________________________________________________

from collections import deque
 
class Graph:
    def __init__(self, adjac_lis):
        self.adjac_lis = adjac_lis
 
    def get_neighbors(self, v):
        return self.adjac_lis[v]
 
    # This is heuristic function which is having equal values for all nodes
    def h(self, n):
        H = {
            'A': 1,
            'B': 1,
            'C': 1,
            'D': 1
        }
 
        return H[n]
 
    def a_star_algorithm(self, start, stop):
        # In this open_lst is a list of nodes which have been visited, but who's 
        # neighbours haven't all been always inspected, It starts off with the start node
        # And closed_lst is a list of nodes which have been visited
        # and who's neighbors have been always inspected
        open_lst = set([start])
        closed_lst = set([])
 
        # poo has present distances from start to all other nodes
        # the default value is +infinity
        poo = {}
        poo[start] = 0
 
        # par contains an adjac mapping of all nodes
        par = {}
        par[start] = start
 
        while len(open_lst) > 0:
            n = None
 
            # it will find a node with the lowest value of f() -
            for v in open_lst:
                if n == None or poo[v] + self.h(v) < poo[n] + self.h(n):
                    n = v;
 
            if n == None:
                print('Path does not exist!')
                return None
 
            # if the current node is the stop
            # then we start again from start
            if n == stop:
                reconst_path = []
 
                while par[n] != n:
                    reconst_path.append(n)
                    n = par[n]
 
                reconst_path.append(start)
 
                reconst_path.reverse()
 
                print('Path found: {}'.format(reconst_path))
                return reconst_path
 
            # for all the neighbors of the current node do
            for (m, weight) in self.get_neighbors(n):
              # if the current node is not presentin both open_lst and closed_lst
                # add it to open_lst and note n as it's par
                if m not in open_lst and m not in closed_lst:
                    open_lst.add(m)
                    par[m] = n
                    poo[m] = poo[n] + weight
 
                # otherwise, check if it's quicker to first visit n, then m
                # and if it is, update par data and poo data
                # and if the node was in the closed_lst, move it to open_lst
                else:
                    if poo[m] > poo[n] + weight:
                        poo[m] = poo[n] + weight
                        par[m] = n
 
                        if m in closed_lst:
                            closed_lst.remove(m)
                            open_lst.add(m)
 
            # remove n from the open_lst, and add it to closed_lst
            # because all of his neighbors were inspected

            open_lst.remove(n)
            closed_lst.add(n)
 
        print('Path does not exist!')
        return None

#Input
adjac_lis = {
    'A': [('B', 1), ('C', 3), ('D', 7)],
    'B': [('D', 5)],
    'C': [('D', 12)]
}
graph1 = Graph(adjac_lis)
graph1.a_star_algorithm('A', 'D')

______________________________________________________________________________

# Python3 Program to print BFS traversal
# from a given source vertex. BFS(int s)
# traverses vertices reachable from s.
from collections import defaultdict

# This class represents a directed graph
# using adjacency list representation
class Graph:

    # Constructor
    def __init__(self):

        # default dictionary to store graph
        self.graph = defaultdict(list)

    # function to add an edge to graph
    def addEdge(self,u,v):
        self.graph[u].append(v)

    # Function to print a BFS of graph
    def BFS(self, s):

        # Mark all the vertices as not visited
        visited = [False] * (max(self.graph) + 1)

        # Create a queue for BFS
        queue = []

        # Mark the source node as
        # visited and enqueue it
        queue.append(s)
        visited[s] = True

        while queue:

            # Dequeue a vertex from
            # queue and print it
            s = queue.pop(0)
            print (s, end = " ")

            # Get all adjacent vertices of the
            # dequeued vertex s. If a adjacent
            # has not been visited, then mark it
            # visited and enqueue it
            for i in self.graph[s]:
                if visited[i] == False:
                    queue.append(i)
                    visited[i] = True

# Driver code

# Create a graph given in
# the above diagram
g = Graph()
g.addEdge(0, 1)
g.addEdge(0, 2)
g.addEdge(1, 2)
g.addEdge(2, 0)
g.addEdge(2, 3)
g.addEdge(3, 3)
g.graph
print("dict", g.graph)
print ("Following is Breadth First Traversal"
                " (starting from vertex 2)")
g.BFS(2)

_________________________________________________________________________________

import matplotlib.pyplot as plt
#matplotlib is used to plot the decision tree
from sklearn.datasets import load_iris
#sklearn.datasets is used to import the dataset
from sklearn import tree
#sklearn tree is used to import decision tree classifier

 
def load_data_set():
  iris = load_iris()
  return iris
 
 
def train_model(iris):
   clf = tree.DecisionTreeClassifier(criterion = "entropy")
   #initilzing the the model with criterion entropy
   clf = clf.fit(iris.data, iris.target)
   #training the model
   return clf
  
def display_image(clf, iris):
    plt.figure()
    plt.figure(figsize=(40, 40))
    tree.plot_tree(clf, feature_names = iris.feature_names,class_names = iris.target_names,filled=True)
    plt.show()


iris_data = load_iris()
#loading the dataset
print(iris_data.feature_names)
print(iris_data.data)
print(iris_data.target_names)
#['setosa' = 0 , 'versicolor' = 1 , 'virginica' = 2]

print(iris_data.target)
decision_tree_classifier = train_model(iris_data)
display_image(decision_tree_classifier, iris_data)
y_pred = decision_tree_classifier.predict([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2],[5.9, 3.0,  5.1, 1.8]])
#[5.1 3.5 1.4 0.2] : 0
#[4.9 3.  1.4 0.2] : 0
#[5.9, 3.0,  5.1, 1.8] : 2

print(y_pred)

______________________________________________________________________________


import numpy as np

import pandas as pd

#Import dataset 

from sklearn import datasets

#Load dataset

wine = datasets.load_wine()

#print(wine)#if you want to see the data you can print data

#print the names of the 13 features

print ("Features: ", wine['feature_names'])

#print the label type of wine

print ("Labels: ", wine.target_names)

X=pd.DataFrame(wine['data'])

print(X)

print(wine.data.shape)

#print the wine labels (0:Class_0, 1:class_2, 2:class_2)

y=print (wine.target)

# Import train_test_split function

from sklearn.model_selection import train_test_split

# Split dataset into training set and test set

X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.30,random_state=109)

#Import Gaussian Naive Bayes model

from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier

gnb = GaussianNB()

#Train the model using the training sets

gnb.fit(X_train, y_train)

#Predict the response for test dataset

y_pred = gnb.predict(X_test)

print(y_pred)

#Import scikit-learn metrics module for accuracy calculation

from sklearn import metrics

# Model Accuracy

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

#confusion matrix 

from sklearn.metrics import  confusion_matrix

cm=np.array(confusion_matrix(y_test,y_pred))

__________________________________________________________________


import sklearn 

import pandas as pd

from sklearn.datasets import load_iris

iris=load_iris()

print(iris.keys())

df=pd.DataFrame(iris['data'])

print(df)

print(iris['target_names'])


iris['feature_names']
print(iris['feature_names'])
X=df

y=iris['target']


#Splitting the data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

#KNN Classifier and Training of the Model
from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(n_neighbors=3,metric = 'cosine')

knn.fit(X_train,y_train)

#Prediction And th Accurancy

import numpy as np

#x_new=np.array([[5,2.9,1,0.2]])
x_new = np.array([[6.3,2.5,5.0,1.9]])
prediction=knn.predict(x_new)
iris['target_names'][prediction]
#print("\n\n",iris['target_names'][prediction])

#Demo
from sklearn.metrics import confusion_matrix

from sklearn.metrics import accuracy_score

from sklearn.metrics import classification_report

y_pred=knn.predict(X_test)

cm=confusion_matrix(y_test,y_pred) 

print(cm)

print(" correct predicition",accuracy_score(y_test,y_pred))

print(" worng predicition",(1-accuracy_score(y_test,y_pred)))

___________________________________________________________________